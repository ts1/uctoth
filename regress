#!/usr/bin/env coffee

require './rejection_handler'
{ readlines, gzwriter, round_value, shuffle } = require './util'
{ get_single_index_size } = require './pattern'
yargs = require 'yargs'
  .usage "Usage: #{process.argv[1]} [options] indexes coeffs"
  .options
    t:
      desc: 'threshold to terminate regression'
      alias: 'threshold'
      default: 1e-3
      type: 'number'
      requiresArg: true
    s:
      alias: 'split'
      desc: 'Split this ratio as test set'
      default: 0
      type: 'number'
      requiresArg: true

argv = yargs.argv

D_FACTOR = 120

index_size = get_single_index_size()

each_index = (row, cb) ->
  for i in [1...row.length] by 2
    cb row[i], row[i+1]

predict = (coeffs, row) ->
  result = 0
  each_index row, (index, value) ->
    result += coeffs[index] * value
  result

verify = (rows, coeffs, upper) ->
  if upper?
    upper = upper * upper * rows.length if upper?
  else
    upper = Infinity
  sum_sq_err = 0
  for row from rows
    outcome = row[0]
    e = outcome - predict(coeffs, row)
    sum_sq_err += e*e
    if sum_sq_err > upper
      break
  Math.sqrt(sum_sq_err / rows.length)

make_gradient = (rows, coeffs) ->
  vector = (0 for i in [0...index_size] by 1)
  sum = 0
  for row from rows
    outcome = row[0]
    e = outcome - predict(coeffs, row)
    each_index row, (index, value) ->
      vector[index] += e * value
  vector

update_coeffs = (coeffs, vector, rate) ->
  coeffs[i] + vector[i] * rate for i in [0...index_size] by 1

read_rows = (filename) ->
  console.log "loading #{filename}"
  rows = []
  await readlines filename, (line) ->
    rows.push JSON.parse line
  if argv.split
    n_train = Math.round(argv.split * rows.length)
    rows = shuffle rows
    train_set = rows.slice(0, n_train)
    test_set = rows.slice(n_train)
    {train_set, test_set}
  else
    train_set = test_set = rows
    {train_set, test_set}

split_rows = (rows) ->

average = (rows) ->
  sum = 0
  for row in rows
    sum += row[0]
  sum / rows.length

regress = (index_file) ->
  {train_set, test_set} = await read_rows index_file

  console.log "train set #{train_set.length}"
  console.log "test set #{test_set.length}"
  avg = average train_set
  console.log "train set avg #{avg}"
  coeffs = (0 for i in [0...index_size] by 1)
  dev = last_loss = verify train_set, coeffs
  console.log 'deviation', last_loss

  n = 0
  step = 1
  d_avg = 0
  rate = 10 / train_set.length
  finish = false
  loop
    vector = make_gradient train_set, coeffs

    try_up = false
    loop
      new_coeffs = update_coeffs coeffs, vector, rate
      loss = verify test_set, new_coeffs, last_loss
      break if loss < last_loss
      rate *= 0.7
      if rate*train_set.length < 1e-5
        if not try_up
          try_up = true
          rate = 1000 / train_set.length
        else
          finish = true
          break
    break if finish
    coeffs = new_coeffs

    d = last_loss - loss
    if step <= D_FACTOR
      d_avg = (d_avg * (step - 1) + d) / step
    else
      d_avg = (d_avg * (D_FACTOR - 1) + d) / D_FACTOR
    console.log 'step', step, 'loss', loss,
      'rate', round_value(rate*train_set.length),
      'd', d_avg
    break if step >= D_FACTOR and d_avg < argv.threshold

    step++

    last_loss = loss

    n++
    if n >= 30
      rate *= 10000
      n = 0

  r2 = 1 - last_loss**2 / dev**2
  console.log 'r2', r2

  {coeffs, r2, step, loss:last_loss, avg}

if argv._.length == 2
  [index_file, outfile] = argv._
else
  yargs.showHelp()
  process.exit 1

do ->
  coeffs = await regress index_file
  gzwriter(outfile).end(JSON.stringify coeffs)
