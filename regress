#!/usr/bin/env coffee

require './rejection_handler'
{ gzwriter, round_value, shuffle } = require './util'
{ get_single_index_size } = require './pattern'
Book = require './book'

argv = require 'yargs'
  .options
    p:
      alias: 'phase'
      desc: 'Phase (0-9)'
      type: 'number'
      requiresArg: true
      demandOption: true
    o:
      alias: 'outfile'
      desc: 'Output file'
      type: 'string'
      requiresArg: true
      demandOption: true
    b:
      alias: 'book'
      desc: 'Database file to read nodes from'
      type: 'string'
      requiresArg: true
      default: 'book.db'
    t:
      desc: 'Threshold to terminate regression'
      alias: 'threshold'
      default: 1e-3
      type: 'number'
      requiresArg: true
    s:
      alias: 'split'
      desc: 'Split this ratio as train set, and rest as test set'
      default: null
      type: 'number'
      requiresArg: true
    d:
      alias: 'duration'
      desc: 'Duration of moving average of difference'
      default: 10
      type: 'number'
      requiresArg: true
    balance:
      desc: 'Thin out samples to get average closer to zero'
      type: 'boolean'
      default: false
    batch:
      desc: 'Use batch'
      type: 'boolean'
      default: false
    sgd:
      desc: 'Use SGD'
      type: 'boolean'
      default: false
    adagrad:
      desc: 'Use AdaGrad'
      type: 'boolean'
      default: true
    rmsprop:
      desc: 'RMSProp parameter (e.g. 0.99)'
      type: 'number'
      default: null
    ridge:
      desc: 'L2 regularization parameter'
      type: 'number'
      default: 0
    h:
      alias: 'help'
  .version false
  .strict()
  .argv

index_size = get_single_index_size()

each_index = (row, cb) ->
  for i in [1...row.length] by 2
    cb row[i], row[i+1]

predict = (coeffs, row) ->
  result = 0
  each_index row, (index, value) ->
    result += coeffs[index] * value
  result

verify = (rows, coeffs, upper) ->
  if upper?
    upper = upper * upper * rows.length if upper?
  else
    upper = Infinity
  sum_sq_err = 0
  for row from rows
    outcome = row[0]
    e = outcome - predict(coeffs, row)
    sum_sq_err += e*e
    if sum_sq_err > upper
      break
  Math.sqrt(sum_sq_err / rows.length)

make_gradient = (rows, coeffs) ->
  vector = (0 for i in [0...index_size] by 1)
  sum = 0
  for row from rows
    outcome = row[0]
    e = outcome - predict(coeffs, row)
    each_index row, (index, value) ->
      vector[index] += e * value
  vector

update_coeffs = (coeffs, vector, rate) ->
  coeffs[i] + vector[i] * rate for i in [0...index_size] by 1

sgd = (rows, coeffs, rate, max_loss) ->
  max_sum_e2 = (max_loss * max_loss) * rows.length
  new_coeffs = [coeffs...]
  sum_e2 = 0
  sum = 0
  for row from rows
    outcome = row[0]
    e = predict(new_coeffs, row) - outcome
    sum_e2 += e * e
    return null if sum_e2 > max_sum_e2
    each_index row, (index, value) ->
      g = e * value
      if argv.ridge
        g += argv.ridge * coeffs[index]
      new_coeffs[index] -= rate * g
  new_coeffs

adagrad = (rows, coeffs, g2, rate, max_loss) ->
  max_sum_e2 = (max_loss * max_loss) * rows.length
  sum_e2 = 0
  sum = 0
  for row from rows
    outcome = row[0]
    e = predict(coeffs, row) - outcome
    sum_e2 += e * e
    return false if sum_e2 > max_sum_e2
    each_index row, (index, value) ->
      g = e * value
      if g
        if argv.ridge
          g += argv.ridge * coeffs[index]
        if argv.rmsprop
          gg = g2[index] = g2[index] * argv.rmsprop + g * g * (1 - argv.rmsprop)
        else
          gg = g2[index] += g * g
        coeffs[index] -= rate * g / Math.sqrt(gg)
  true

binary_search = (samples, target) ->
  lbound = 0
  return lbound if target <= samples[lbound][0]
  ubound = samples.length - 1
  return ubound if target >= samples[ubound][0]
  while lbound < ubound
    mid = Math.round((lbound + ubound) / 2)
    value = samples[mid][0]
    return mid if value == target
    if value < target
      lbound = mid + 1
    else
      ubound = mid - 1
  mid

thin_out = (samples) ->
  balance = 0
  for sample in samples
    balance += sample[0]
  #console.log 'balance', balance
  orig_balance = balance
  samples.sort (a, b) -> a[0] - b[0]
  n_removed = 0
  while balance != 0
    #console.log 'balance', balance
    i = binary_search(samples, balance)
    value = samples[i][0]
    if value == 0
      #console.log 'abount to kill zero'
      break
    #console.log 'killing', value
    samples.splice i, 1
    n_removed++
    balance -= value
  console.log 'thinned out', n_removed, 'samples'
  {samples, n_removed}

split_rows = (rows, ratio) ->
  n_train = Math.round(ratio * rows.length)
  rows = shuffle rows
  train_set = rows.slice(0, n_train)
  test_set = rows.slice(n_train)
  {train_set, test_set}

average = (rows) ->
  sum = 0
  for row in rows
    sum += row[0]
  sum / rows.length

regress = (samples) ->
  n_samples = samples.length
  raw_avg = average samples
  console.log 'samples avg', raw_avg

  if argv.balance
    {samples, n_removed} = thin_out(samples)
  if argv.split
    {train_set, test_set} = split_rows(samples, argv.split)
  else
    train_set = test_set = samples

  console.log "train set #{train_set.length} samples"
  console.log "test set #{test_set.length} samples"
  avg = average train_set
  console.log "train set avg #{avg}"
  coeffs = (0 for i in [0...index_size] by 1)
  best_coeffs = [coeffs...]
  dev = last_loss = verify train_set, coeffs
  console.log 'deviation', dev

  n = 0
  step = 1
  d_avg = 0
  rate = 100
  rate /= train_set.length if argv.batch
  finish = false
  min_loss = dev
  loop
    if argv.adagrad
      train_set = shuffle train_set
      if step == 1
        last = false
        loop
          g2 = (0 for i in [0...index_size] by 1)
          coeffs = (0 for i in [0...index_size] by 1)
          if adagrad(train_set, coeffs, g2, rate, dev)
            loss = verify test_set, coeffs, last_loss
            if loss < last_loss
              if last
                break
              rate *= 0.1 / 0.7
              last = true
          rate *= 0.7
          if rate < 1e-4
            finish = true
            break
      else
        adagrad(train_set, coeffs, g2, rate, Infinity)
        loss = verify test_set, coeffs, last_loss
        if argv.split
          break if loss > min_loss * (1 + argv.threshold)
    else if argv.sgd
      train_set = shuffle train_set
      loop
        max_loss = if step==1 then last_loss else Infinity
        new_coeffs = sgd(train_set, coeffs, rate, max_loss)
        if new_coeffs
          loss = verify test_set, new_coeffs, last_loss
          if loss < last_loss
            break
        train_set = shuffle train_set
        rate *= 0.7
        if rate < 1e-10
          finish = true
          break
      coeffs = new_coeffs
    else
      vector = make_gradient train_set, coeffs

      loop
        new_coeffs = update_coeffs coeffs, vector, rate
        loss = verify test_set, new_coeffs, last_loss
        break if loss < last_loss
        rate *= 0.7
        if rate*train_set.length < 1e-5
          finish = true
          break
      coeffs = new_coeffs
    break if finish

    if loss < min_loss
      min_loss = loss
      best_coeffs = [coeffs...]

    d = Math.abs(last_loss - loss)
    if step <= argv.duration
      d_avg = (d_avg * (step - 1) + d) / step
    else
      d_avg = (d_avg * (argv.duration - 1) + d) / argv.duration

    display_rate = rate
    unless argv.adagrad
      display_rate *= train_set.length
    console.log 'step', step, 'loss', loss,
      'rate', round_value(display_rate),
      'd', d_avg,
      if loss == min_loss then '*' else ''

    break if step >= argv.duration and d_avg < argv.threshold

    step++

    last_loss = loss

    if argv.batch
      n++
      if n >= 30
        rate *= 10000
        n = 0

  r2 = 1 - last_loss**2 / dev**2
  console.log 'r2', r2

  coeffs = best_coeffs
  loss = min_loss
  {coeffs, r2, step, loss, avg, n_samples, raw_avg, n_removed}

do ->
  argv.adagrad = argv.sgd = false if argv.batch
  argv.adagrad = argv.batch = false if argv.sgd
  argv.adagrad = true if argv.rmsprop
  argv.sgd = argv.batch = false if argv.adagrad

  book = new Book argv.book
  book.init()

  process.stdout.write "Loading samples of phase #{argv.phase}: "
  t = Date.now()
  samples = []
  samples.push sample for sample from book.iterate_indexes(argv.phase)
  t = Date.now() - t
  console.log "loaded #{samples.length} samples in #{t/1000} seconds"

  coeffs = regress samples

  process.stdout.write "Writing coeffs to #{argv.outfile}: "
  gzwriter(argv.outfile).end(JSON.stringify coeffs)
  console.log 'done'
