#!/usr/bin/env coffee

{ get_single_index_size, code_to_single_indexes, SCORE_MULT } =
  require './pattern'
{ shuffle } = require './util'
Book = require './book'
fs = require 'fs'

yargs = require 'yargs'
  .options
    p:
      alias: 'phase'
      desc: 'Game phase (0-9)'
      type: 'number'
      requiresArg: true
      demandOption: true
    o:
      alias: 'outfile'
      desc: 'Output file'
      type: 'string'
      requiresArg: true
    b:
      alias: 'book'
      desc: 'Database file'
      default: 'book.db'
      requiresArg: true
    B:
      alias: 'batch_size'
      desc: 'Batch size (default auto)'
      requiresArg: true
    e:
      alias: 'epochs'
      desc: 'Number of epochs'
      default: 100
      requiresArg: true
    r:
      alias: 'rate'
      desc: 'Learning rate (default auto)'
      requiresArg: true
    l:
      alias: 'logistic'
      desc: 'Logistic regression'
    l2:
      desc: 'L2 regularization parameter'
      requiresArg: true
    cv:
      desc: 'K-fold cross validation'
      type: 'number'
      requiresArg: true
    search:
      desc: 'Search optimal L2 parameter'
      type: 'boolean'
    search_precision:
      desc: 'Precision of L2 parameter search (>1)'
      type: 'number'
      default: 1.1
      requiresArg: true
    h:
      alias: 'help'
  .strict()
  .version false

argv = yargs.argv

INDEX_SIZE = get_single_index_size()

load_samples = () ->
  for {code, outcome} from book.fetch_codes_for_phase(argv.phase)
    [outcome, code_to_single_indexes(code)...]

clip = do ->
  MAX = 32767 / SCORE_MULT
  MIN = -32768 / SCORE_MULT
  (x) ->
    if x > MAX then MAX
    else if x < MIN then MIN
    else x

predict = do ->
  do_predict = (indexes, coeffs) ->
    result = 0
    for i in [1...indexes.length] by 2
      index = indexes[i]
      value = indexes[i+1]
      result += coeffs[index] * value
    result

  if argv.logistic
    (indexes, coeffs) -> 1 / (1 + Math.exp(-do_predict(indexes, coeffs)))
  else
    do_predict

make_gradient = (batch, coeffs) ->
  gradient = (0 for i in [0...INDEX_SIZE])
  e2 = 0
  for indexes in batch
    outcome = indexes[0]
    e = predict(indexes, coeffs) - outcome
    for i in [1...indexes.length] by 2
      index = indexes[i]
      value = indexes[i+1]
      g = e * value
      gradient[index] -= g
    e2 += e * e
  {gradient, e2}

epoch = (samples, coeffs, g2, rate, batch_size, max_loss=Infinity) ->
  max_e2 = max_loss**2 * samples.length
  sum_e2 = 0
  n_batches = Math.ceil(samples.length / batch_size)
  for offset in [0...n_batches]
    batch = (samples[i] for i in [offset...samples.length] by n_batches)
    {gradient, e2} = make_gradient(batch, coeffs)
    sum_e2 += e2
    return false if sum_e2 >= max_e2
    for i in [0...INDEX_SIZE]
      g = gradient[i]
      continue unless g
      g2[i] += g * g
      w = coeffs[i]
      if argv.l2
        g -= argv.l2 * w
      w += rate/Math.sqrt(g2[i]) * g
      coeffs[i] = clip(w)
  return true

verify = (samples, coeffs) ->
  e2 = 0
  for indexes in samples
    outcome = indexes[0]
    e = predict(indexes, coeffs) - outcome
    e2 += e * e
  Math.sqrt(e2 / samples.length)

find_rate = (samples, batch_size, dev) ->
  # rough tune
  min_loss = dev
  best_rate = null
  rate = 100
  n_fail = 0
  step_size = 10
  loop
    coeffs = (0 for i in [0...INDEX_SIZE])
    g2 = (0 for i in [0...INDEX_SIZE])
    if epoch(samples, coeffs, g2, rate, batch_size, dev)
      loss = verify(samples, coeffs)
      if loss < min_loss
        min_loss = loss
        best_rate = rate
      else
        if best_rate?
          if ++n_fail >= 2
            break
    else
      break if best_rate?
    rate /= step_size

  # fine tune
  while step_size >= 1.01
    step_size **= .5

    rate = best_rate * step_size
    coeffs = (0 for i in [0...INDEX_SIZE])
    g2 = (0 for i in [0...INDEX_SIZE])
    epoch samples, coeffs, g2, rate, batch_size
    loss = verify(samples, coeffs)
    if loss < min_loss
      min_loss = loss
      best_rate = rate
      continue

    rate = best_rate / step_size
    coeffs = (0 for i in [0...INDEX_SIZE])
    g2 = (0 for i in [0...INDEX_SIZE])
    epoch samples, coeffs, g2, rate, batch_size
    loss = verify(samples, coeffs)
    if loss < min_loss
      min_loss = loss
      best_rate = rate

  best_rate

train = (samples, {quiet}={}) ->
  dev = verify(samples, (0 for i in [0...INDEX_SIZE]))
  console.log "Deviation: #{dev}" unless quiet

  if argv.batch_size?
    batch_size = argv.batch_size
  else
    batch_size = Math.round(samples.length / 10)
    console.log "Batch size: #{batch_size}" unless quiet

  if argv.rate?
    rate = argv.rate
  else
    process.stdout.write 'Finding optimal learning rate: ' unless quiet
    rate = find_rate(samples, batch_size, dev)
    console.log "#{rate}" unless quiet

  coeffs = (0 for i in [0...INDEX_SIZE])
  g2 = (0 for i in [0...INDEX_SIZE])
  min_loss = dev
  for ep in [1..argv.epochs]
    epoch shuffle(samples), coeffs, g2, rate, batch_size
    loss = verify(samples, coeffs)
    if loss < min_loss
      min_loss = loss
      best_coeffs = [coeffs...]
      star = '*'
    else
      star = ''
    console.log "epoch #{ep} loss #{loss} #{star}" unless quiet
  {coeffs: best_coeffs, loss: min_loss, dev}

split_groups = (samples, k) ->
  win_set = shuffle(samples.filter (sample) -> sample[0] > 0)
  loss_set = shuffle(samples.filter (sample) -> sample[0] <= 0)

  win_per_group = Math.round(win_set.length / k)
  win_groups =
    for i in [0...k]
      win_set[i*win_per_group...(i + 1)*win_per_group]

  loss_per_group = Math.round(loss_set.length / k)
  loss_groups =
    for i in [0...k]
      loss_set[i*win_per_group...(i + 1)*loss_per_group]

  groups =
    for i in [0...k]
      shuffle(win_groups[i].concat(loss_groups[i]))

  groups

cross_validation = (groups, {quiet}={}) ->
  k = groups.length
  avg_loss = 0
  for i in [0...k]
    test_set = groups[i]
    train_set = []
    for j in [0...k]
      train_set = train_set.concat(groups[j]) unless j==i
    console.log "Cross Validation #{i+1}/#{k}" unless quiet
    {coeffs} = train(train_set, quiet: true)

    loss = verify(test_set, coeffs)
    console.log "test set loss: #{loss}" unless quiet
    avg_loss += loss
  avg_loss /= k
  console.log "loss average: #{avg_loss}" unless quiet
  avg_loss

test_l2 = (groups, l2, min) ->
  process.stdout.write "l2 #{l2}: "
  argv.l2 = l2
  loss = cross_validation(groups, quiet: true)
  process.stdout.write "loss #{loss}"
  process.stdout.write ' *' if loss < min
  process.stdout.write '\n'
  loss

search_l2 = (groups) ->
  ubound = argv.l2 or 1
  lbound = 0
  step = 10
  l2 = ubound
  min_loss = Infinity
  best = null
  n_fail = 0
  loop
    loss = test_l2(groups, l2, min_loss)

    if loss < min_loss
      min_loss = loss
      best = l2
      n_fail = 0
    else
      n_fail++
    l2 /= step
    
    break if l2 < lbound or (not lbound and n_fail >= 1)

  while step > argv.search_precision
    step **= 1/2
    l2 = best

    tmp = l2 * step
    loss = test_l2(groups, tmp, min_loss)
    if loss < min_loss
      min_loss = loss
      best = tmp
    else
      tmp = l2 / step
      loss = test_l2(groups, tmp, min_loss)
      if loss < min_loss
        min_loss = loss
        best = tmp

  console.log "Best L2: #{best}"
  best

do ->
  book = new Book argv.book
  book.init()

  process.stdout.write "Loading samples for phase #{argv.phase}: "
  samples = shuffle(sample for sample from book.iterate_indexes(argv.phase))
  if argv.logistic
    samples.forEach (sample) -> sample[0] = if sample[0] > 0 then 1 else 0
  console.log "loaded #{samples.length} samples"
  avg = samples.reduce(((a, s) -> a + s[0]), 0) / samples.length
  console.log "Average: #{avg}"

  if argv.cv or argv.search
    groups = split_groups(samples, argv.cv or 4)

  if argv.search
    argv.l2 = search_l2(groups)
    if argv.outfile?
      fs.writeFileSync argv.outfile, "#{argv.l2}\n"
  else if argv.cv
    cross_validation(groups)
  else
    if not argv.outfile?
      yargs.showHelp()
      console.error '--outfile is required'
      process.exit 1

    {coeffs, loss, dev, r2} = train(samples)

    r2 = 1 - loss**2 / dev**2
    console.log "r2: #{r2}"

    output = {
      coeffs
      avg
      dev
      r2
      loss
      logistic: argv.logistic
      l2: argv.l2
    }
    process.stdout.write "Writing #{argv.outfile}: "
    fs.writeFileSync argv.outfile, JSON.stringify output
    console.log "done"
