#!/usr/bin/env coffee

require './rejection_handler'
{ PatternBoard, patterns, N_MOVES_PER_PHASE, N_PHASES } = require './pattern'
{ EMPTY } = require './board'
{ gzwriter, int } = require './util'
{ decode } = require './encode'
Book = require './book'

argv = require('yargs')
  .options
    b:
      alias: 'book'
      desc: 'Database file'
      default: 'book.db'
    o:
      alias: 'outdir'
      desc: 'Output directory'
      default: 'tmp'
    h:
      alias: 'help'
  .strict()
  .version false
  .argv

indexify = (book, outfiles) ->
  LIMIT = 1000
  n_indexed = 0
  offset = 0
  last_key = ''
  loop
    n = 0
    rows = book.dump_nodes_with_indexes LIMIT, last_key
    for {code, outcome, indexes} in rows
      board = new PatternBoard decode code
      unless indexes
        indexes = []
        for p in patterns
          map = {}
          for i in p.indexes
            index = p.normalize(board.indexes[i])
            abs = Math.abs index
            value = map[abs] or 0
            if index >= 0
              map[abs] = value + 1
            else
              map[abs] = value - 1
          for index, value of map
            console.assert p.get_single_index(index) != null
            indexes.push p.get_single_index(index)
            indexes.push value
        book.store_indexes code, indexes
        n_indexed++
      phase = int((59 - board.count(EMPTY)) / N_MOVES_PER_PHASE)
      json = JSON.stringify([outcome, indexes...])
      for ph in [phase-1..phase+1]
        if ph >= 0 and ph < N_PHASES
          outfiles[ph].write "#{json}\n"
      n++
      last_key = code
    break unless n
    offset += n
    process.stdout.write "#{offset}\r"
  console.log "dumped #{offset} nodes (#{n_indexed} new nodes)"

do ->
  console.log "dumping indexes from #{argv.book}"
  book = new Book argv.book
  book.init()
  outfiles = (gzwriter "#{argv.outdir}/indexes#{i}" for i in [0...N_PHASES])
  indexify book, outfiles
  outfiles.forEach (f) -> f.end()
